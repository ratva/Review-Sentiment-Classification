{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from load_train_data import load_data \n",
    "from clean_data import tokenize_text, create_word_list, make_feature_vector\n",
    "import sklearn.model_selection\n",
    "import sklearn.feature_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from nltk import word_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_list, vocab_list, rating_list = load_data('x_train.csv', 'y_train.csv')\n",
    "tokens = tokenize_text(vocab_list)\n",
    "word_count_dict = create_word_list(tokens)\n",
    "print(word_count_dict)\n",
    "\n",
    "#threshold of words that are infrequent is 3 right now. Change later(?)\n",
    "sorted_tokens = list(sorted(word_count_dict, key=word_count_dict.get, reverse=True))\n",
    "# vocab_list = [w for w in sorted_tokens if word_count_dict[w] >= 3]\n",
    "print('sorted tokens:', sorted_tokens)\n",
    "print('rating_list:', rating_list)\n",
    "\n",
    "bow_processor = CountVectorizer(binary = false, vocabulary=word_count_dict)\n",
    "#make_feature_vector(a_review, word_count_dict)\n",
    "print(bow_processor.vocabulary)\n",
    "pos_review = 0\n",
    "if rating_list == 1:\n",
    "    pos_review += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_cleaner = CountVectorizer(ngram_range=(1,1), min_df=1, max_df=1.0, tokenizer=True, vocabulary=None)\n",
    "bow_cleaner.fit(rating_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to run a unit test to run on the analysis functions, but didn't get it working yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "from analysis import cross_validation, clean_words\n",
    "\n",
    "class TestAnalysis(unittest.TestCase):\n",
    "    \n",
    "    def test_cross_validation(self):\n",
    "        # Test that test_ids_fold and train_ids_fold are lists\n",
    "        x_NF = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "        y_N = np.array([0, 1, 0, 1])\n",
    "        n_folds = 2\n",
    "        test_ids_fold, train_ids_fold = cross_validation(x_NF, y_N, n_folds)\n",
    "        self.assertIsInstance(test_ids_fold, list)\n",
    "        self.assertIsInstance(train_ids_fold, list)\n",
    "        \n",
    "        # Test that test_ids_fold and train_ids_fold have the correct length\n",
    "        self.assertEqual(len(test_ids_fold), n_folds)\n",
    "        self.assertEqual(len(train_ids_fold), n_folds)\n",
    "        \n",
    "        # Test that test_ids_fold and train_ids_fold contain the correct indices\n",
    "        for i in range(n_folds):\n",
    "            test_ids = test_ids_fold[i]\n",
    "            train_ids = train_ids_fold[i]\n",
    "            self.assertEqual(len(test_ids), len(train_ids))\n",
    "            self.assertEqual(set(test_ids).union(set(train_ids)), set(range(len(x_NF))))\n",
    "    \n",
    "    def test_clean_words(self):\n",
    "        # Test that y_hat is a sparse matrix\n",
    "        web_list = ['www.example.com', 'www.example.com', 'www.example.com']\n",
    "        reviews_list = ['This is a great product', 'This product is terrible', 'I love this product']\n",
    "        stars_list = [5, 1, 5]\n",
    "        n_folds = 2\n",
    "        y_hat = clean_words(web_list, reviews_list, stars_list, 5)\n",
    "        \n",
    "        self.assertIsInstance(y_hat, scipy.sparse.csr.csr_matrix)\n",
    "        \n",
    "        # Test that y_hat has the correct shape\n",
    "        self.assertEqual(y_hat.shape, (3, len(reviews_list)))\n",
    "        \n",
    "        # Test that y_hat contains the correct values\n",
    "        expected_y_hat = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "        np.testing.assert_array_equal(y_hat.toarray(), expected_y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_analysis = TestAnalysis()\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(test_analysis)\n",
    "\n",
    "test_result = unittest.TextTestRunner(verbosity=2).run(suite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading example from online to see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.linear_model as sklm\n",
    "from scipy.stats import uniform\n",
    "\n",
    "iris = load_iris()\n",
    "logistic = sklm.LogisticRegression(solver='saga', tol=1e-2, max_iter=200,random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
    "clf = skms.RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "search = clf.fit(iris.data, iris.target)\n",
    "search.best_params_\n",
    "print(search.best_params_)\n",
    "print(search.classes_)\n",
    "print(search.best_estimator_)\n",
    "print(search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.linear_model as sklm\n",
    "import sklearn.pipeline as pipe\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.feature_selection \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from load_train_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "website_list, vocab_list, rating_list = load_data('x_train.csv', 'y_train.csv')\n",
    "cleaned_tr = CountVectorizer(vocabulary=rating_list, min_df=3)\n",
    "print(cleaned_tr.vocabulary)\n",
    "#cleaned_te = CountVectorizer(vocabulary=)\n",
    "\n",
    "\n",
    "bow_classifier = sklearn.pipeline.Pipeline([\n",
    "    ('bow feature extractor', CountVectorizer(ngram_range=(1,1), min_df=1, max_df=1.0, binary=False)),\n",
    "    ('classifying', sklm.LogisticRegression(C = 1.0, max_iter=20, random_state=101))])\n",
    "\n",
    "\n",
    "\n",
    "bow_classifier.fit(vocab_list, rating_list)\n",
    "# bow_classifier.predict()\n",
    "# print(bow_classifier.stop_words)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
