{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as sklm\n",
    "import sklearn.pipeline\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.feature_selection \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Import our filess\n",
    "from load_train_data import load_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_list, review_list, rating_list = load_data('x_train.csv', 'y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leigh\\micromamba\\envs\\cs135_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 62 is smaller than n_iter=100. Running 62 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logistic = sklm.LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "distributions = dict(C=np.logspace(-9,6,31), penalty = ['l2', 'l1'])\n",
    "\n",
    "#Pipeline starts!\n",
    "my_bow_classifier_pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('my_bow_feature_extractor', CountVectorizer(min_df=1, max_df=1.0, ngram_range=(1,1))),\n",
    "    ('cross validation', skms.RandomizedSearchCV(logistic, distributions, n_iter=100, cv=10, verbose=0, random_state=0, error_score='raise', return_train_score=True))\n",
    "])\n",
    "\n",
    "my_bow_classifier_pipeline.fit(review_list, rating_list)\n",
    "my_bow_classifier_pipeline.predict(review_list)\n",
    "my_bow_classifier_pipeline.score(review_list, rating_list)\n",
    "probs = my_bow_classifier_pipeline.predict_proba(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'C': 1.0}\n",
      "Training accuracy: 0.994\n"
     ]
    }
   ],
   "source": [
    "weights = my_bow_classifier_pipeline['cross validation'].best_estimator_.coef_\n",
    "\n",
    "#getting CountVectorizer dictionary\n",
    "dictionary = my_bow_classifier_pipeline['my_bow_feature_extractor'].vocabulary_\n",
    "\n",
    "print(my_bow_classifier_pipeline['cross validation'].best_params_)\n",
    "\n",
    "acc = roc_auc_score(rating_list, probs[:,1])\n",
    "print(\"Training accuracy: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te_data = 'x_test.csv'\n",
    "data_dir = 'data_reviews'\n",
    "x_te_df = pd.read_csv(os.path.join(data_dir, x_te_data))\n",
    "te_website_list = x_te_df['website_name'].values.tolist()\n",
    "te_text_list = x_te_df['text'].values.tolist()\n",
    "\n",
    "probs = my_bow_classifier_pipeline.predict_proba(te_text_list)[:, 1]\n",
    "print(probs)\n",
    "\n",
    "np.savetxt('q1.txt', probs, fmt='%s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
