{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as sklm\n",
    "import sklearn.pipeline\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.feature_selection \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Import our filess\n",
    "from load_train_data import load_data \n",
    "from clean_data import tokenize_text, create_word_list, create_dict, make_feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_list, review_list, rating_list = load_data('x_train.csv', 'y_train.csv')\n",
    "tokens = tokenize_text(review_list) #A list of 28459 cleaned words\n",
    "# print(len(tokens))\n",
    "\n",
    "word_count_dict = create_word_list(tokens) # dictionary of 4883 unique word counts\n",
    "sorted_tokens = list(sorted(word_count_dict, key=word_count_dict.get, reverse=True))\n",
    "vocab_dict = create_dict(sorted_tokens)\n",
    "\n",
    "# clean up the data INSTEAD of tokens = tokenize_text(review_list)\n",
    "vectorizer = CountVectorizer(stop_words = 'english', ngram_range=(1,1), min_df=3, max_df=0.08, binary=False, vocabulary=vocab_dict) #filter out words over 0.08 occurence\n",
    "# vectorizer = CountVectorizer(analyzer = 'word',tokenizer=lambda txt: txt.split(),token_pattern = str,ngram_range=(1,1), min_df=0.0, max_df=1.0, binary=False)\n",
    "\n",
    "X = vectorizer.fit(review_list, rating_list) \n",
    "\n",
    "# Save output to text file\n",
    "with open('wordFreq.txt', 'w') as f:\n",
    "    f.write(str(word_count_dict))\n",
    "\n",
    "with open('afterCountVectorizer.txt', 'w') as f:\n",
    "    # f.write(str(word_count_dict))\n",
    "    f.write(str(vectorizer.get_feature_names_out().tolist()))\n",
    "\n",
    "\n",
    "# with open('output.txt', 'r') as f:\n",
    "#     print(f.read())\n",
    "\n",
    "#print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorizer.get_feature_names_out()[703])\n",
    "#print(review_list)\n",
    "# print(vectorizer.get_feature_names_out()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted_tokens = list(sorted(word_count_dict, key=word_count_dict.get, reverse=True))\n",
    "\n",
    "print(type(sorted_tokens)) # list of tokens sorted by frequency\n",
    "\n",
    "# Print all words in list and their frequencies from the dictionary\n",
    "for w in sorted_tokens:\n",
    "    print(\"%5d %s\" % (word_count_dict[w], w))\n",
    "    \n",
    "# Create a dictionary of the sorted tokens list\n",
    "vocab_dict = create_dict(sorted_tokens)\n",
    "print(sorted_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh and I forgot to also mention the weird color effect it has on your phone.\n",
      "Oh and I forgot to also mention the weird color effect it has on your phone.\n",
      "16.0\n",
      "THAT one didn't work either.\n",
      "THAT one didn't work either.\n",
      "5.0\n",
      "Waste of 13 bucks.\n",
      "Waste of 13 bucks.\n",
      "4.0\n",
      "Product is useless, since it does not have enough charging current to charge the 2 cellphones I was planning to use it with.\n",
      "Product is useless, since it does not have enough charging current to charge the 2 cellphones I was planning to use it with.\n",
      "23.0\n",
      "None of the three sizes they sent with the headset would stay in my ears.\n",
      "None of the three sizes they sent with the headset would stay in my ears.\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "# Turning reviews into feature vectors\n",
    "N = len(review_list)\n",
    "V = len(sorted_tokens)\n",
    "\n",
    "x_tr_NV = np.zeros((N,V))   # N x V matrix of feature vectors\n",
    "\n",
    "\n",
    "for nn, review_line in enumerate(review_list[0:5]):\n",
    "    x_tr_NV[nn] = make_feature_vector(review_line, vocab_dict)\n",
    "    print(review_line)\n",
    "    print(sum(x_tr_NV[nn]))\n",
    "    # print(review_list[nn])\n",
    "# TODO are we using the right inputs and functions here?\n",
    "# with open('x_tr_feature.txt', 'w') as f:\n",
    "#     # f.write(str(word_count_dict))\n",
    "#     f.write(str(x_tr_NV[5].tolist()))\n",
    "\n",
    "# print(np.sum(x_tr_NV[5]))\n",
    "# print(review_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.994\n"
     ]
    }
   ],
   "source": [
    "my_bow_classifier_pipeline = sklearn.pipeline.Pipeline([\n",
    "    ('my_bow_feature_extractor', CountVectorizer(min_df=1, max_df=1.0, ngram_range=(1,1))),\n",
    "    ('my_classifier', sklm.LogisticRegression(C=1.0, max_iter=200, random_state=101)),\n",
    "])\n",
    "\n",
    "my_bow_classifier_pipeline.fit(review_list, rating_list)\n",
    "my_bow_classifier_pipeline.predict(review_list)\n",
    "my_bow_classifier_pipeline.score(review_list, rating_list)\n",
    "probs = my_bow_classifier_pipeline.predict_proba(review_list)\n",
    "acc1 = roc_auc_score(rating_list, probs[:,1])\n",
    "print(\"Training accuracy: %.3f\" % acc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.model_selection.RandomizedSearchCV\n",
    "Look for if there is an industry standard for what % of your training data should be the number of iterations and just use that and explain that's what you considered instead of evaluating that you have reached an optimum minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random search cross validation to generate optimum C, penalty and max_iter for a logistic regression classifier\n",
    "logistic = sklm.LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "distributions = dict(C=np.logspace(-9,-6,31), penalty = ['l2', 'l1'])\n",
    "randClassifier = skms.RandomizedSearchCV(logistic, distributions, n_iter=100, cv=10, verbose=0, random_state=0, error_score='raise', return_train_score=True)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Tried to add shuffle before passing it into the randomizedSearchCV to see if it would improve the fit. It basically didn't - tiny improvement.\n",
    "# x_tr_NV, rating_list = shuffle(x_tr_NV, rating_list, random_state=0)\n",
    "#print(type(randClassifier))\n",
    "#randClassifier.fit(x_tr_NV, rating_list)\n",
    "# TODO ensure we're passing the right x and y variables and that they match each other (right ratings with each review) into the classifier.\n",
    "\n",
    "#print(type(randClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.502\n"
     ]
    }
   ],
   "source": [
    "newregress = sklm.LogisticRegression()\n",
    "newregress.fit(x_tr_NV, rating_list)\n",
    "yhat_tr_N = newregress.predict(x_tr_NV)\n",
    "acc_tr = np.mean(yhat_tr_N == rating_list)\n",
    "\n",
    "print(\"Training accuracy: %.3f\" % acc_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just experimenting to ensure that if I pass 2 arrays in that shuffle shuffles both in the same way\n",
    "# a = np.arange(18)\n",
    "# b = np.arange(18)+7\n",
    "\n",
    "# a, b = shuffle(a, b, random_state=0)\n",
    "\n",
    "# print(a)\n",
    "# print(b-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'randClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leigh\\cs135\\projectA\\q1.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leigh/cs135/projectA/q1.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Probability of 0 and 1 [2400,2]\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/leigh/cs135/projectA/q1.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m yhat \u001b[39m=\u001b[39m randClassifier\u001b[39m.\u001b[39mpredict(x_tr_NV)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leigh/cs135/projectA/q1.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m((yhat\u001b[39m.\u001b[39mshape))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leigh/cs135/projectA/q1.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Save yhat to text file\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'randClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Probability of 0 and 1 [2400,2]\n",
    "yhat = randClassifier.predict(x_tr_NV)\n",
    "print((yhat.shape))\n",
    "\n",
    "# Save yhat to text file\n",
    "with open('yhat.txt', 'w') as f:\n",
    "    f.write(str(yhat.tolist()))\n",
    "\n",
    "acccuracy = np.mean(yhat == rating_list)\n",
    "print(\"Training accuracy = %f\" % acccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.502\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the AUROC accuracy of the classifier on the training data\n",
    "acc = roc_auc_score(rating_list, yhat[:,])\n",
    "print(\"Training accuracy: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_feature_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import uniform\n",
    "# iris = load_iris()\n",
    "# logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200, random_state=0)\n",
    "# distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
    "# clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "# search = clf.fit(iris.data, iris.target)\n",
    "# search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
